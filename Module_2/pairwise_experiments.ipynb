{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new task! Here, we have a salesperson named Bob. Bob has a lot of deals, so he wants to summarize what happened in this deals based off of some meeting transcripts.\n",
    "\n",
    "Bob is iterating on a few different prompts, that will give him nice, concise transcripts for his deals.\n",
    "\n",
    "Bob has curated a dataset of his deal transcripts, let's go ahead and load that in. You can take a look at the dataset as well if you're curious! Note that this is not a golden dataset, there is no reference output here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "  \"https://smith.langchain.com/public/9078d2f1-7bef-4ba7-b795-210a17682ef9/d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some experiments on this dataset using two different prompts. Let's add an evaluator that tries to score how good our summaries are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "SUMMARIZATION_SYSTEM_PROMPT = \"\"\"You are a judge, aiming to score how well a summary summarizes the content of a transcript\"\"\"\n",
    "\n",
    "SUMMARIZATION_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "[The Start of Summarization] {summary} [The End of Summarization]\"\"\"\n",
    "\n",
    "class SummarizationScore(BaseModel):\n",
    "    score: int = Field(description=\"\"\"A score from 1-5 ranking how good the summarization is for the provided transcript, with 1 being a bad summary, and 5 being a great summary\"\"\")\n",
    "    \n",
    "def summary_score_evaluator(inputs: dict, outputs: dict) -> list:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": SUMMARIZATION_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
    "                    transcript=inputs[\"transcript\"],\n",
    "                    summary=outputs.get(\"output\", \"N/A\"),\n",
    "                )}\n",
    "        ],\n",
    "        response_format=SummarizationScore,\n",
    "    )\n",
    "\n",
    "    summary_score = completion.choices[0].message.parsed.score\n",
    "    return {\"key\": \"summary_score\", \"score\": summary_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run our experiment with a good version of our prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Good Summarizer-d44413d7' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=dffb4c99-93c9-4faa-a3c2-fc806bd8b5d1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0731a425c15f40aeab6518522803ee31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>Bob and Ms. Nguyen discussed various vehicle o...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5.513180</td>\n",
       "      <td>695abda5-75ac-4e22-ad72-16e15b849568</td>\n",
       "      <td>49a30e6c-4e50-42fd-b606-abd595d58a47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Bob and Mr. Patel discussed Mr. Patel's intere...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2.179189</td>\n",
       "      <td>76f8d0a9-a479-4771-80cd-8bb689440137</td>\n",
       "      <td>0371b7a0-1208-491b-9422-9f28bbe9d568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Bob introduced the Ford Explorer to Mr. Johnso...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2.287355</td>\n",
       "      <td>7790f742-19f7-401b-bd7d-5f23bcc496ff</td>\n",
       "      <td>84ac03a5-07ae-4dc7-ae41-a6004c1e7d3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Bob and Mr. Carter met to discuss a potential ...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2.241295</td>\n",
       "      <td>a0487f72-abbb-42d4-b8d1-348ce6a53680</td>\n",
       "      <td>979d0f69-d35d-4ed6-bbc4-b4922c475b53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Bob welcomed Ms. Thompson to Ford Motors, and ...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2.443839</td>\n",
       "      <td>b1d5ecd4-5d0b-40bd-8688-6c90bf8e290e</td>\n",
       "      <td>bddb761a-defe-4779-a8ca-4fec643e65a9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Good Summarizer-d44413d7>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt One: Good Prompt!\n",
    "def good_summarizer(inputs: dict):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Concisely summarize this meeting in 3 sentences. Make sure to include all of the important events. Meeting: {inputs['transcript']}\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "client.evaluate(\n",
    "    good_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Good Summarizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run an experiment with a worse version of our prompt, to highlight the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Bad Summarizer-f1ebf283' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=4b996ad5-312e-4383-8e9f-3eefbc0c8347\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a166aee2b29c4d108b7cbd198af1011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>Ms. Nguyen explores vehicle options.</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1.635795</td>\n",
       "      <td>695abda5-75ac-4e22-ad72-16e15b849568</td>\n",
       "      <td>4170df76-ff32-4ce9-9857-efe3c80613af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Bob sells Mr. Patel car.</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1.092796</td>\n",
       "      <td>76f8d0a9-a479-4771-80cd-8bb689440137</td>\n",
       "      <td>309244e3-7b70-4e23-8207-a3065c81f3dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Bob sells Mr. Johnson SUV.</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.715854</td>\n",
       "      <td>7790f742-19f7-401b-bd7d-5f23bcc496ff</td>\n",
       "      <td>6b887d47-1d50-4081-ba60-a01125e8cf59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Trade-in, upgrade, test drive, deal.</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.950897</td>\n",
       "      <td>a0487f72-abbb-42d4-b8d1-348ce6a53680</td>\n",
       "      <td>1240656a-05b5-4dbd-8e60-8d05623a657c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Customer browsing, no purchase made.</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1.639357</td>\n",
       "      <td>b1d5ecd4-5d0b-40bd-8688-6c90bf8e290e</td>\n",
       "      <td>4d56437d-a8ec-4eb0-8971-1460685d536d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Bad Summarizer-f1ebf283>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Two: Worse Prompt!\n",
    "def bad_summarizer(inputs: dict):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize this in 5 words. {inputs['transcript']}\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "client.evaluate(\n",
    "    bad_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Bad Summarizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will compare our two experiments. These are the fields that pairwise evaluator functions get access to:\n",
    "- `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n",
    "- `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.\n",
    "- `reference_outputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n",
    "- `runs: list[Run]`: A list of the full Run objects generated by the experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\n",
    "- `example: Example`: The full dataset Example, including the example inputs, outputs (if available), and metdata (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's give our LLM-as-Judge some instructions. In our case, we're just going to directly use LLM-as-judge to grade which of the summarizers is the most helpful.\n",
    "\n",
    "It might be hard to grade our summarizers without a ground truth reference, but here, comparing different prompts head to head will give us a sense of which is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the summarizations provided by two AI summarizers to the meeting transcript below.\n",
    "Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their summarizations. \n",
    "Begin your evaluation by comparing the two summarizations and provide a short explanation. \n",
    "Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. \n",
    "Do not favor certain names of the assistants. \n",
    "Be as objective as possible. \"\"\"\n",
    "\n",
    "JUDGE_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "\n",
    "[The Start of Assistant A's Summarization] {answer_a} [The End of Assistant A's Summarization]\n",
    "\n",
    "[The Start of Assistant B's Summarization] {answer_b} [The End of Assistant B's Summarization]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function will take in an `inputs` dictionary, and a list of `outputs` dictionaries for the different experiments that we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Preference(BaseModel):\n",
    "    preference: int = Field(description=\"\"\"1 if Assistant A answer is better based upon the factors above.\n",
    "2 if Assistant B answer is better based upon the factors above.\n",
    "Output 0 if it is a tie.\"\"\")\n",
    "    \n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": JUDGE_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": JUDGE_HUMAN_PROMPT.format(\n",
    "                    transcript=inputs[\"transcript\"],\n",
    "                    answer_a=outputs[0].get(\"output\", \"N/A\"),\n",
    "                    answer_b=outputs[1].get(\"output\", \"N/A\")\n",
    "                )}\n",
    "        ],\n",
    "        response_format=Preference,\n",
    "    )\n",
    "\n",
    "    preference_score = completion.choices[0].message.parsed.preference\n",
    "\n",
    "    if preference_score == 1:\n",
    "        scores = [1, 0]\n",
    "    elif preference_score == 2:\n",
    "        scores = [0, 1]\n",
    "    else:\n",
    "        scores = [0, 0]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our pairwise experiment with `evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with temperature=0.0\n",
      "View the evaluation results for experiment: 'Temperature-0.0-925983b1' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=1f0b9faf-d86b-49ef-95c5-4f3033594288\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281f6b70e57e4bdea1f485ddd7ce6752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created: Temperature-0.0-925983b1\n",
      "\n",
      "Running experiment with temperature=0.3\n",
      "View the evaluation results for experiment: 'Temperature-0.3-e3203024' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=241cb9c2-6b4e-4818-a2cb-f783ee87ada9\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2fcd9facdd405ca2936b1e16616d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created: Temperature-0.3-e3203024\n",
      "\n",
      "Running experiment with temperature=0.7\n",
      "View the evaluation results for experiment: 'Temperature-0.7-bececa5b' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=9f7090b1-9c2d-4dbe-abd7-a41c432b3f2c\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5338edab0194dc880928179f1a8fe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created: Temperature-0.7-bececa5b\n",
      "\n",
      "Running experiment with temperature=1.0\n",
      "View the evaluation results for experiment: 'Temperature-1.0-8126a76e' at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=549486e8-b36e-4308-a880-aabab7908bb5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd5453cfeb8456aa09942c9c6ef9ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created: Temperature-1.0-8126a76e\n",
      "\n",
      "==================================================\n",
      "All temperature experiments completed!\n",
      "==================================================\n",
      "\n",
      "Comparing Temperature-0.0-925983b1 vs Temperature-0.7-bececa5b...\n",
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/4ffb8661-bc93-48d9-919b-d7dd87377bc1/datasets/e6569a50-edac-42db-a0d6-37fe682e871d/compare?selectedSessions=1f0b9faf-d86b-49ef-95c5-4f3033594288%2C9f7090b1-9c2d-4dbe-abd7-a41c432b3f2c&comparativeExperiment=ce96206e-6691-464d-850e-4e80667960b0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3372290ae142ef85c026274bfe34d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise comparison completed!\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Testing Different Temperature Settings\n",
    "\n",
    "TEMPERATURES = [0.0, 0.3, 0.7, 1.0]\n",
    "experiment_results = {}\n",
    "\n",
    "for temp in TEMPERATURES:\n",
    "    print(f\"\\nRunning experiment with temperature={temp}\")\n",
    "    \n",
    "    def temp_summarizer(inputs: dict, temperature=temp):\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Concisely summarize this meeting in 3 sentences. Make sure to include all of the important events. Meeting: {inputs['transcript']}\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    result = client.evaluate(\n",
    "        temp_summarizer,\n",
    "        data=dataset,\n",
    "        evaluators=[summary_score_evaluator],\n",
    "        experiment_prefix=f\"Temperature-{temp}\",\n",
    "        metadata={\"temperature\": temp}\n",
    "    )\n",
    "    \n",
    "    # Store the experiment name (which includes a unique ID)\n",
    "    experiment_results[temp] = result.experiment_name\n",
    "    print(f\"Experiment created: {result.experiment_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All temperature experiments completed!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Now compare using the actual experiment names\n",
    "print(f\"\\nComparing {experiment_results[0.0]} vs {experiment_results[0.7]}...\")\n",
    "evaluate(\n",
    "    (experiment_results[0.0], experiment_results[0.7]),\n",
    "    evaluators=[ranked_preference],\n",
    "    experiment_prefix=\"Pairwise: Temp 0.0 vs 0.7\"\n",
    ")\n",
    "\n",
    "print(\"\\nPairwise comparison completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
