{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm not sure what you're asking. Could you please provide more context or clarify your question about trace setters?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"Trace setter question?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    # messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith with the @traceable decorator in Python, decorate any function you want to trace with @traceable. Ensure that the LANGSMITH_TRACING environment variable is set to 'true' and provide your LANGSMITH_API_KEY in the environment. After this setup, you can start logging traces by executing your decorated functions.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Pulling prompt from hub...\n",
      "Successfully pulled prompt: nik-hil:a7fc7883\n",
      "Current prompt content:\n",
      "first=ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'nik-hil', 'lc_hub_commit_hash': 'a7fc788350b817e97b4372f7057aab64687cec45b61d421632b439f9cc249aed'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the latest question in the conversation. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Context: Trace query syntax - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationData type referenceTrace query syntaxGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubTry LangSmithOn this pageFilter argumentsFilter query languageData type referenceTrace query syntaxCopy pageCopy pageUsing the method in the SDK or endpoint in the API, you can filter runs to analyze and export.\\n\\u200bFilter arguments\\n\\nFilter traces - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationViewing & managing tracesFilter tracesGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubTry LangSmithOn this pageCreating and Applying FiltersFiltering by run attributesFiltering by time rangeFilter operatorsSpecific Filtering TechniquesFilter for intermediate runs (spans)Filter based on inputs and outputsFilter based on input / output key-value pairsExample: Filtering for tool callsNegative filtering on key-value pairsSave a filterSave a filterUse a saved filterUpdate a saved filterDelete a saved filterCopy a filterFiltering runs within the trace viewManually specify a raw query in LangSmith query languageUse an AI Query to auto-generate a query (Experimental)Advanced filtersFilter for intermediate runs (spans) on properties of the rootFilter for runs (spans) whose child runs have some attributeExample: Filtering on all runs whose tree contains the tool call filterViewing & managing tracesFilter tracesCopy pageCopy pageRecommended reading: It might be helpful to read the Conceptual guide on tracing to gain familiarity with the concepts mentioned on this page.\\n\\nAnnotate traces and runs inline - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationAnnotation & human feedbackAnnotate traces and runs inlineGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubTry LangSmithAnnotation & human feedbackAnnotate traces and runs inlineCopy pageCopy pageLangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user’s comment or a note about a specific issue.\\nYou can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.\\nFeedback tags are associated with your workspace.\\nYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.\\nTo annotate a trace inline, click on the Annotate in the upper right corner of trace view for any particular run that is part of the trace.\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They’re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedback—whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. Why would I upgrade a base trace to an extended trace?\\n\\nBase traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They’re priced for volume and short-term utility.\\u200dExtended traces, on the other hand, are retained for 400 days and often include valuable feedback—whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.\\u200dLangSmith lets you choose the right retention for each trace, helping you balance cost and value. I’ve exhausted my free trace allocation. What can I do?\\n\\nIf you’ve used up your free traces, you can input your credit card details on the Developer or Plus plans to continue sending traces to LangSmith. If you’ve hit the performance usage limits on your tier, you can upgrade to a higher plan to get better limits, or reach out to support@langchain.dev with questions.LangGraph Platform QuestionsDoes LangGraph Platform include any free deployments?\\n\\nPlus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, you’ll be charged by usage (nodes executed and uptime).For my free Dev deployment for LangGraph Platform, is there a cap on the number of nodes executed?\\n\\nIf you’re on the Plus plan, you get 1 free dev-sized deployment – all usage in this deployment will be free no matter how many node executions are run.How do you define nodes executed? \\n\\n Question: Trace setter question?'), additional_kwargs={})]) middle=[] last=RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002666410A710>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002666410AAD0>, root_client=<openai.OpenAI object at 0x0000026664109A90>, root_async_client=<openai.AsyncOpenAI object at 0x000002666410A850>, model_name='gpt-4o-mini', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), kwargs={}, config={}, config_factories=[])\n",
      "\n",
      "Step 2: Creating improved version of the prompt...\n",
      "Improved prompt:\n",
      "You are a technical assistant specializing in LangSmith.\n",
      "Answer questions using ONLY the provided context.\n",
      "Be concise and direct - maximum 2 sentences.\n",
      "If the context doesn't contain the answer, say \"I don't have that information in the provided context.\"\n",
      "\n",
      "Context: {context}\n",
      "Question: {question}\n",
      "\n",
      "Step 3: Pushing improved prompt to hub...\n",
      "Error pushing prompt: Client.push_prompt() takes 2 positional arguments but 3 were given\n",
      "Note: Make sure you have write permissions for this prompt\n",
      "\n",
      "Step 4: Usage in your RAG application:\n",
      "\n",
      "Replace the hardcoded RAG_SYSTEM_PROMPT in generate_response with:\n",
      "\n",
      "formatted_prompt = prompt.invoke({\"context\": formatted_docs, \"question\": question})\n",
      "messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt Hub Lifecycle Demo: Pull, Modify, Push\n",
    "\n",
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "# Initialize client (uses LANGSMITH_API_KEY from environment)\n",
    "client = Client()\n",
    "\n",
    "# Step 1: Pull existing prompt from hub\n",
    "print(\"Step 1: Pulling prompt from hub...\")\n",
    "prompt_identifier = \"nik-hil:a7fc7883\"  # Your existing prompt\n",
    "pulled_prompt = client.pull_prompt(prompt_identifier, include_model=True)\n",
    "print(f\"Successfully pulled prompt: {prompt_identifier}\")\n",
    "print(f\"Current prompt content:\\n{pulled_prompt}\\n\")\n",
    "\n",
    "# Step 2: Create an improved version\n",
    "print(\"Step 2: Creating improved version of the prompt...\")\n",
    "improved_prompt = \"\"\"You are a technical assistant specializing in LangSmith.\n",
    "Answer questions using ONLY the provided context.\n",
    "Be concise and direct - maximum 2 sentences.\n",
    "If the context doesn't contain the answer, say \"I don't have that information in the provided context.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "print(f\"Improved prompt:\\n{improved_prompt}\\n\")\n",
    "\n",
    "# Step 3: Push the improved version to hub\n",
    "print(\"Step 3: Pushing improved prompt to hub...\")\n",
    "try:\n",
    "    # This will create a new version of your prompt\n",
    "    new_prompt = client.push_prompt(\n",
    "        prompt_identifier.split(':')[0],  # Uses the same prompt name\n",
    "        improved_prompt\n",
    "    )\n",
    "    print(f\"Successfully pushed improved prompt!\")\n",
    "    print(f\"New version can be accessed with the same identifier\")\n",
    "except Exception as e:\n",
    "    print(f\"Error pushing prompt: {e}\")\n",
    "    print(\"Note: Make sure you have write permissions for this prompt\")\n",
    "\n",
    "# Step 4: Show how to use it in the application\n",
    "print(\"\\nStep 4: Usage in your RAG application:\")\n",
    "print(\"\"\"\n",
    "Replace the hardcoded RAG_SYSTEM_PROMPT in generate_response with:\n",
    "\n",
    "formatted_prompt = prompt.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
